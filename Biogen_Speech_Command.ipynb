{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Biogen Speech Command",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefancosquer/deep-speech-command/blob/master/Biogen_Speech_Command.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "OlNaO2tUYB9A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Intro"
      ]
    },
    {
      "metadata": {
        "id": "o6fmIG-1YPhg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Objectif"
      ]
    },
    {
      "metadata": {
        "id": "7y-I2R-OYFZK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Reconnaitre les chiffres de 1 à 9 depuis un flux audio continu."
      ]
    },
    {
      "metadata": {
        "id": "0zsO0O6sYUhX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Etapes\n",
        "\n",
        "*  **Création du dataset** : ensemble de fichiers wave contenant la prononciation d'un seul chiffre.\n",
        "*  **Entrainement du réseau** de neurones avec Keras\n",
        "*  **Conversion du réseau** vers TensorFlow Lite\n",
        "*  **Création d'un module React Native** réalisant la capture audio et l'inférence"
      ]
    },
    {
      "metadata": {
        "id": "NPjfBxwo6Y93",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Création du dataset"
      ]
    },
    {
      "metadata": {
        "id": "fqIOcl3Nyvg6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install python3-pyaudio\n",
        "!pip install pyaudio\n",
        "\n",
        "import os\n",
        "os.makedirs('speech/recordings')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oy-_5Khi6P5N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pyaudio\n",
        "import math\n",
        "import time\n",
        "import wave\n",
        "import os\n",
        "from scipy.io.wavfile import read, write\n",
        "from collections import defaultdict\n",
        "\n",
        "FORMAT = pyaudio.paInt16\n",
        "CHANNELS = 1\n",
        "SAMPLERATE = 44100\n",
        "BUFFER = 1024\n",
        "DELAY_BETWEEN_NUMBERS = 3\n",
        "REPEATS_PER_NUMBER = 1\n",
        "\n",
        "p = pyaudio.PyAudio()\n",
        "\n",
        "name = input(\"Votre nom: \")\n",
        "\n",
        "recording = 'speech/recordings/speech_' + name + '.wav'\n",
        "wavefile = wave.open(recording, 'wb')\n",
        "wavefile.setnchannels(CHANNELS)\n",
        "wavefile.setsampwidth(pyaudio.get_sample_size(FORMAT))\n",
        "wavefile.setframerate(SAMPLERATE)\n",
        "\n",
        "\n",
        "def record(in_data, frame_count, time_info, status_flags):\n",
        "    wavefile.writeframes(in_data)\n",
        "    return (None, pyaudio.paContinue)\n",
        "\n",
        "\n",
        "def record_numbers():\n",
        "    nums = [str(i) for i in range(1, 10) for set_num in range(REPEATS_PER_NUMBER)]\n",
        "    for i in range(len(nums)):\n",
        "        target = int(round(math.pi * i)) % len(nums)\n",
        "        (nums[i], nums[target]) = (nums[target], nums[i])\n",
        "\n",
        "    stream = p.open(format=FORMAT, channels=CHANNELS, rate=SAMPLERATE,\n",
        "                    input=True, output=False,\n",
        "                    frames_per_buffer=BUFFER,\n",
        "                    stream_callback=record)\n",
        "\n",
        "    print(\"Pret ?\")\n",
        "    time.sleep(DELAY_BETWEEN_NUMBERS)\n",
        "\n",
        "    for num in nums:\n",
        "        print(num)\n",
        "        time.sleep(DELAY_BETWEEN_NUMBERS)\n",
        "\n",
        "    stream.stop_stream()\n",
        "    stream.close()\n",
        "    p.terminate()\n",
        "    wavefile.close()\n",
        "\n",
        "    return nums\n",
        "\n",
        "\n",
        "def trim(data):\n",
        "    start = 0\n",
        "    end = len(data) - 1\n",
        "\n",
        "    mag = abs(data)\n",
        "    thresold = mag.max() * 0.2\n",
        "\n",
        "    for idx, point in enumerate(mag):\n",
        "        if point > thresold:\n",
        "            start = max(start, idx - 4410)\n",
        "            break\n",
        "\n",
        "    for idx, point in enumerate(mag[::-1]):\n",
        "        if point > thresold:\n",
        "            end = min(end, len(data) - idx + 4410)\n",
        "            break\n",
        "\n",
        "    return data[start:end]\n",
        "\n",
        "\n",
        "def split(numbers):\n",
        "\n",
        "    rate, data = read(recording)\n",
        "\n",
        "    counts = defaultdict(lambda: 0)\n",
        "\n",
        "    for i, label in enumerate(numbers):\n",
        "        label = str(label)\n",
        "        start_idx = (i + 1) * int(SAMPLERATE * DELAY_BETWEEN_NUMBERS)\n",
        "        stop_idx = start_idx + int(SAMPLERATE * DELAY_BETWEEN_NUMBERS)\n",
        "\n",
        "        digit = data[start_idx:stop_idx]\n",
        "        digit = trim(digit)\n",
        "\n",
        "        write('speech/recordings' + os.sep + label + \"_\" + name + \"_\" + str(counts[label]) + '.wav', SAMPLERATE, digit)\n",
        "\n",
        "        counts[label] += 1\n",
        "\n",
        "    os.remove(recording)\n",
        "\n",
        "\n",
        "split(record_numbers())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dRyV-m4V1DXV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Apprentissage"
      ]
    },
    {
      "metadata": {
        "id": "u0_mrM7gd6YN",
        "colab_type": "code",
        "outputId": "91db44e5-db21-4c49-ddc9-fd00e08ff8fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        }
      },
      "cell_type": "code",
      "source": [
        "!rm -rf deep-speech-command\n",
        "!git clone https://github.com/stefancosquer/deep-speech-command.git"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deep-speech-command'...\n",
            "remote: Enumerating objects: 35, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 35 (delta 7), reused 32 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (35/35), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PxJ7Kc7bd7LP",
        "colab_type": "code",
        "outputId": "66a71dac-d84d-4561-b161-7bd9793d4acc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5511
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import MaxPool1D, Conv1D, Dropout, BatchNormalization, GlobalAvgPool1D\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from scipy.io.wavfile import read, write\n",
        "\n",
        "SAMPLERATE = 44100\n",
        "CLASSES = ['-', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(64, 88, activation='relu', input_shape=(SAMPLERATE, 1)))\n",
        "model.add(MaxPool1D(3))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(64, 4, activation='relu'))\n",
        "model.add(MaxPool1D(3))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(128, 4, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(GlobalAvgPool1D())\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "## Train\n",
        "\n",
        "def generator(directory, batch_size, shuffle=False):\n",
        "    rate, background = read('deep-speech-command/backgrounds/hospital.wav')\n",
        "    files = [file for file in os.listdir(directory) if file.endswith(\".wav\")]\n",
        "    np.random.shuffle(files)\n",
        "    x = np.zeros((batch_size, SAMPLERATE, 1))\n",
        "    y = np.zeros((batch_size, len(CLASSES)))\n",
        "    while True:\n",
        "        if (shuffle):\n",
        "            np.random.shuffle(files)\n",
        "        for i in range(batch_size):\n",
        "            # choose random file\n",
        "            rate, samples = read(directory + '/' + files[i % len(files)])\n",
        "            # pad randomly before and after\n",
        "            before = 1 # random.randint(0, SAMPLERATE - samples.size)\n",
        "            after = SAMPLERATE - samples.size - before\n",
        "            samples = np.concatenate((np.zeros(before), samples, np.zeros(after)))\n",
        "            # mix with background sounds\n",
        "            idx = np.random.randint(0, len(background) - SAMPLERATE)\n",
        "            samples = samples + np.random.random() * background[idx:idx + SAMPLERATE]\n",
        "            # normalize sound\n",
        "            samples = (samples - np.mean(samples)) / np.std(samples)\n",
        "            x[i] = samples.reshape(1, SAMPLERATE, 1)\n",
        "            y[i] = to_categorical(files[i % len(files)][0], len(CLASSES))\n",
        "        yield x, y\n",
        "\n",
        "history = model.fit_generator(\n",
        "    generator('deep-speech-command/recordings', 40, shuffle=True),\n",
        "    validation_data=generator('deep-speech-command/recordings', 20),\n",
        "    steps_per_epoch=1,\n",
        "    validation_steps=1,\n",
        "    epochs=1000, \n",
        "    callbacks=[EarlyStopping(patience=100, restore_best_weights=True)],\n",
        "    verbose=1)\n",
        "\n",
        "## Evaluate\n",
        "\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "## Test\n",
        "\n",
        "def test(file):\n",
        "    rate, samples = read('deep-speech-command/recordings/' + file)\n",
        "    samples = (samples - np.mean(samples)) / np.std(samples)\n",
        "    samples = np.concatenate((samples, np.zeros(SAMPLERATE - samples.size)))\n",
        "    samples = samples.reshape(1, SAMPLERATE, 1)\n",
        "    predictions = model.predict(samples)\n",
        "    print(file + ' : ' + str(np.argmax(predictions)) + ' (' + str(np.amax(predictions) * 100) + '%)')\n",
        "\n",
        "\n",
        "test('0_stefan_0.wav')\n",
        "test('1_stefan_0.wav')\n",
        "test('2_stefan_0.wav')\n",
        "test('3_stefan_0.wav')\n",
        "test('4_stefan_0.wav')\n",
        "test('5_stefan_0.wav')\n",
        "test('6_stefan_0.wav')\n",
        "test('7_stefan_0.wav')\n",
        "test('8_stefan_0.wav')\n",
        "test('9_stefan_0.wav')\n",
        "\n",
        "model.save('model.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_115 (Conv1D)          (None, 44013, 64)         5696      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_78 (MaxPooling (None, 14671, 64)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_115 (Bat (None, 14671, 64)         256       \n",
            "_________________________________________________________________\n",
            "dropout_115 (Dropout)        (None, 14671, 64)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_116 (Conv1D)          (None, 14668, 64)         16448     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_79 (MaxPooling (None, 4889, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_116 (Bat (None, 4889, 64)          256       \n",
            "_________________________________________________________________\n",
            "dropout_116 (Dropout)        (None, 4889, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_117 (Conv1D)          (None, 4886, 128)         32896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_117 (Bat (None, 4886, 128)         512       \n",
            "_________________________________________________________________\n",
            "dropout_117 (Dropout)        (None, 4886, 128)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_38  (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 57,354\n",
            "Trainable params: 56,842\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            "1/1 [==============================] - 16s 16s/step - loss: 2.2542 - acc: 0.2000 - val_loss: 2.0344 - val_acc: 0.4000\n",
            "Epoch 2/1000\n",
            "1/1 [==============================] - 0s 432ms/step - loss: 2.0524 - acc: 0.3750 - val_loss: 1.9853 - val_acc: 0.3500\n",
            "Epoch 3/1000\n",
            "1/1 [==============================] - 0s 426ms/step - loss: 1.9219 - acc: 0.4000 - val_loss: 1.7972 - val_acc: 0.6500\n",
            "Epoch 4/1000\n",
            "1/1 [==============================] - 0s 430ms/step - loss: 1.9057 - acc: 0.4250 - val_loss: 1.7888 - val_acc: 0.5000\n",
            "Epoch 5/1000\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 1.9618 - acc: 0.3250 - val_loss: 1.6723 - val_acc: 0.5500\n",
            "Epoch 6/1000\n",
            "1/1 [==============================] - 0s 426ms/step - loss: 1.6743 - acc: 0.4500 - val_loss: 1.6148 - val_acc: 0.3500\n",
            "Epoch 7/1000\n",
            "1/1 [==============================] - 0s 427ms/step - loss: 1.7304 - acc: 0.4250 - val_loss: 1.6665 - val_acc: 0.4000\n",
            "Epoch 8/1000\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 1.5680 - acc: 0.4500 - val_loss: 1.6310 - val_acc: 0.4000\n",
            "Epoch 9/1000\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 1.5703 - acc: 0.4750 - val_loss: 1.6004 - val_acc: 0.4500\n",
            "Epoch 10/1000\n",
            "1/1 [==============================] - 0s 426ms/step - loss: 1.6039 - acc: 0.5250 - val_loss: 1.4642 - val_acc: 0.5000\n",
            "Epoch 11/1000\n",
            "1/1 [==============================] - 0s 426ms/step - loss: 1.6206 - acc: 0.4500 - val_loss: 1.5615 - val_acc: 0.4000\n",
            "Epoch 12/1000\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 1.4251 - acc: 0.4500 - val_loss: 1.5486 - val_acc: 0.3500\n",
            "Epoch 13/1000\n",
            "1/1 [==============================] - 0s 427ms/step - loss: 1.5030 - acc: 0.4500 - val_loss: 1.4492 - val_acc: 0.6500\n",
            "Epoch 14/1000\n",
            "1/1 [==============================] - 0s 426ms/step - loss: 1.7017 - acc: 0.5500 - val_loss: 1.2879 - val_acc: 0.8000\n",
            "Epoch 15/1000\n",
            "1/1 [==============================] - 0s 426ms/step - loss: 1.4661 - acc: 0.5750 - val_loss: 1.2862 - val_acc: 0.5500\n",
            "Epoch 16/1000\n",
            "1/1 [==============================] - 0s 427ms/step - loss: 1.4559 - acc: 0.5750 - val_loss: 1.3086 - val_acc: 0.4500\n",
            "Epoch 17/1000\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 1.3376 - acc: 0.6750 - val_loss: 1.3650 - val_acc: 0.5000\n",
            "Epoch 18/1000\n",
            "1/1 [==============================] - 0s 438ms/step - loss: 1.3161 - acc: 0.6500 - val_loss: 1.1742 - val_acc: 0.5500\n",
            "Epoch 19/1000\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 1.3566 - acc: 0.6750 - val_loss: 1.1834 - val_acc: 0.5500\n",
            "Epoch 20/1000\n",
            "1/1 [==============================] - 0s 426ms/step - loss: 1.3673 - acc: 0.7500 - val_loss: 1.2632 - val_acc: 0.5000\n",
            "Epoch 21/1000\n",
            "1/1 [==============================] - 0s 436ms/step - loss: 1.2616 - acc: 0.7250 - val_loss: 1.1712 - val_acc: 0.6500\n",
            "Epoch 22/1000\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 1.3384 - acc: 0.7000 - val_loss: 1.0954 - val_acc: 0.6500\n",
            "Epoch 23/1000\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 1.4020 - acc: 0.7000 - val_loss: 1.1091 - val_acc: 0.7000\n",
            "Epoch 24/1000\n",
            "1/1 [==============================] - 0s 433ms/step - loss: 1.3297 - acc: 0.6750 - val_loss: 1.1501 - val_acc: 0.7000\n",
            "Epoch 25/1000\n",
            "1/1 [==============================] - 0s 430ms/step - loss: 1.3861 - acc: 0.7500 - val_loss: 1.1210 - val_acc: 0.6500\n",
            "Epoch 26/1000\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 1.1487 - acc: 0.7750 - val_loss: 1.0864 - val_acc: 0.8000\n",
            "Epoch 27/1000\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 1.4351 - acc: 0.6500 - val_loss: 1.0357 - val_acc: 0.7500\n",
            "Epoch 28/1000\n",
            "1/1 [==============================] - 0s 427ms/step - loss: 1.1580 - acc: 0.8250 - val_loss: 1.2387 - val_acc: 0.5500\n",
            "Epoch 29/1000\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 1.2600 - acc: 0.7750 - val_loss: 1.0750 - val_acc: 0.7500\n",
            "Epoch 30/1000\n",
            "1/1 [==============================] - 0s 430ms/step - loss: 1.1643 - acc: 0.7250 - val_loss: 1.0248 - val_acc: 0.7500\n",
            "Epoch 31/1000\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 1.1136 - acc: 0.8250 - val_loss: 0.9513 - val_acc: 0.7500\n",
            "Epoch 32/1000\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 1.1122 - acc: 0.8000 - val_loss: 1.1902 - val_acc: 0.5500\n",
            "Epoch 33/1000\n",
            "1/1 [==============================] - 0s 432ms/step - loss: 1.1370 - acc: 0.8000 - val_loss: 0.9945 - val_acc: 0.6000\n",
            "Epoch 34/1000\n",
            "1/1 [==============================] - 0s 440ms/step - loss: 1.1757 - acc: 0.7750 - val_loss: 1.1327 - val_acc: 0.6500\n",
            "Epoch 35/1000\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 1.2740 - acc: 0.7500 - val_loss: 1.1220 - val_acc: 0.7000\n",
            "Epoch 36/1000\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 1.1560 - acc: 0.7750 - val_loss: 1.0833 - val_acc: 0.6000\n",
            "Epoch 37/1000\n",
            "1/1 [==============================] - 0s 439ms/step - loss: 1.0789 - acc: 0.7500 - val_loss: 0.9667 - val_acc: 0.6500\n",
            "Epoch 38/1000\n",
            "1/1 [==============================] - 0s 432ms/step - loss: 0.9936 - acc: 0.8500 - val_loss: 1.2867 - val_acc: 0.5500\n",
            "Epoch 39/1000\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 1.0560 - acc: 0.8000 - val_loss: 1.0315 - val_acc: 0.6000\n",
            "Epoch 40/1000\n",
            "1/1 [==============================] - 0s 427ms/step - loss: 1.2134 - acc: 0.7000 - val_loss: 0.9120 - val_acc: 0.6000\n",
            "Epoch 41/1000\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 1.2492 - acc: 0.7500 - val_loss: 0.9894 - val_acc: 0.5500\n",
            "Epoch 42/1000\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 1.0891 - acc: 0.7500 - val_loss: 1.0874 - val_acc: 0.5000\n",
            "Epoch 43/1000\n",
            "1/1 [==============================] - 0s 432ms/step - loss: 1.2202 - acc: 0.7000 - val_loss: 0.9468 - val_acc: 0.6000\n",
            "Epoch 44/1000\n",
            "1/1 [==============================] - 0s 430ms/step - loss: 1.1978 - acc: 0.7500 - val_loss: 0.9902 - val_acc: 0.6500\n",
            "Epoch 45/1000\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 1.0690 - acc: 0.8000 - val_loss: 0.8233 - val_acc: 0.7500\n",
            "Epoch 46/1000\n",
            "1/1 [==============================] - 0s 440ms/step - loss: 1.2376 - acc: 0.7500 - val_loss: 0.9161 - val_acc: 0.6000\n",
            "Epoch 47/1000\n",
            "1/1 [==============================] - 0s 432ms/step - loss: 1.1265 - acc: 0.7750 - val_loss: 0.9174 - val_acc: 0.6500\n",
            "Epoch 48/1000\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 1.0290 - acc: 0.7750 - val_loss: 0.8274 - val_acc: 0.6500\n",
            "Epoch 49/1000\n",
            "1/1 [==============================] - 0s 430ms/step - loss: 0.8024 - acc: 0.9500 - val_loss: 0.7275 - val_acc: 0.8000\n",
            "Epoch 50/1000\n",
            "1/1 [==============================] - 0s 430ms/step - loss: 0.9548 - acc: 0.8000 - val_loss: 1.0090 - val_acc: 0.8000\n",
            "Epoch 51/1000\n",
            "1/1 [==============================] - 0s 432ms/step - loss: 1.1066 - acc: 0.8250 - val_loss: 0.8140 - val_acc: 0.8000\n",
            "Epoch 52/1000\n",
            "1/1 [==============================] - 0s 433ms/step - loss: 1.3107 - acc: 0.7000 - val_loss: 0.9181 - val_acc: 0.8000\n",
            "Epoch 53/1000\n",
            "1/1 [==============================] - 0s 436ms/step - loss: 0.9372 - acc: 0.8000 - val_loss: 0.9527 - val_acc: 0.8500\n",
            "Epoch 54/1000\n",
            "1/1 [==============================] - 0s 430ms/step - loss: 0.7375 - acc: 0.8750 - val_loss: 0.6920 - val_acc: 1.0000\n",
            "Epoch 55/1000\n",
            "1/1 [==============================] - 0s 432ms/step - loss: 1.1688 - acc: 0.8000 - val_loss: 0.8297 - val_acc: 0.9000\n",
            "Epoch 56/1000\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 1.0000 - acc: 0.8750 - val_loss: 0.7528 - val_acc: 0.9000\n",
            "Epoch 57/1000\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 0.8496 - acc: 0.8250 - val_loss: 0.6871 - val_acc: 0.9500\n",
            "Epoch 58/1000\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 1.2183 - acc: 0.8000 - val_loss: 0.6948 - val_acc: 0.9000\n",
            "Epoch 59/1000\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 1.0074 - acc: 0.8500 - val_loss: 1.0308 - val_acc: 0.7000\n",
            "Epoch 60/1000\n",
            "1/1 [==============================] - 0s 435ms/step - loss: 0.7475 - acc: 0.8500 - val_loss: 0.7151 - val_acc: 0.8500\n",
            "Epoch 61/1000\n",
            "1/1 [==============================] - 0s 436ms/step - loss: 0.9872 - acc: 0.8250 - val_loss: 0.7271 - val_acc: 0.8000\n",
            "Epoch 62/1000\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 0.7403 - acc: 0.9000 - val_loss: 0.8610 - val_acc: 0.8000\n",
            "Epoch 63/1000\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 1.0252 - acc: 0.8250 - val_loss: 0.7252 - val_acc: 0.9000\n",
            "Epoch 64/1000\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 0.6377 - acc: 0.9500 - val_loss: 0.6792 - val_acc: 0.8000\n",
            "Epoch 65/1000\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 0.8769 - acc: 0.9000 - val_loss: 0.7428 - val_acc: 0.9000\n",
            "Epoch 66/1000\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 0.8774 - acc: 0.8750 - val_loss: 0.5721 - val_acc: 0.9500\n",
            "Epoch 67/1000\n",
            "1/1 [==============================] - 0s 426ms/step - loss: 0.6987 - acc: 0.9250 - val_loss: 0.7410 - val_acc: 0.8500\n",
            "Epoch 68/1000\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 0.5692 - acc: 0.9500 - val_loss: 1.1388 - val_acc: 0.7000\n",
            "Epoch 69/1000\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 0.6374 - acc: 0.9250 - val_loss: 0.8445 - val_acc: 0.7500\n",
            "Epoch 70/1000\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 0.6566 - acc: 0.9000 - val_loss: 0.6281 - val_acc: 0.8000\n",
            "Epoch 71/1000\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 0.6571 - acc: 0.9250 - val_loss: 0.6652 - val_acc: 0.8500\n",
            "Epoch 72/1000\n",
            "1/1 [==============================] - 0s 432ms/step - loss: 0.5136 - acc: 0.9500 - val_loss: 0.6843 - val_acc: 0.9000\n",
            "Epoch 73/1000\n",
            "1/1 [==============================] - 0s 436ms/step - loss: 0.6301 - acc: 0.8750 - val_loss: 0.6482 - val_acc: 0.9500\n",
            "Epoch 74/1000\n",
            "1/1 [==============================] - 0s 432ms/step - loss: 0.6765 - acc: 0.9000 - val_loss: 0.6128 - val_acc: 0.9000\n",
            "Epoch 75/1000\n",
            "1/1 [==============================] - 0s 436ms/step - loss: 0.7064 - acc: 0.9250 - val_loss: 0.8669 - val_acc: 0.8000\n",
            "Epoch 76/1000\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 0.7299 - acc: 0.8250 - val_loss: 0.5440 - val_acc: 0.9500\n",
            "Epoch 77/1000\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 0.7401 - acc: 0.8750 - val_loss: 0.7295 - val_acc: 0.8000\n",
            "Epoch 78/1000\n",
            "1/1 [==============================] - 0s 430ms/step - loss: 0.8188 - acc: 0.8750 - val_loss: 0.6721 - val_acc: 0.9000\n",
            "Epoch 79/1000\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 0.7780 - acc: 0.8750 - val_loss: 0.6174 - val_acc: 0.9000\n",
            "Epoch 80/1000\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 0.6587 - acc: 0.9000 - val_loss: 0.4976 - val_acc: 0.9000\n",
            "Epoch 81/1000\n",
            "1/1 [==============================] - 0s 435ms/step - loss: 1.4015 - acc: 0.7250 - val_loss: 0.4979 - val_acc: 0.9500\n",
            "Epoch 82/1000\n",
            "1/1 [==============================] - 0s 427ms/step - loss: 0.4377 - acc: 0.9250 - val_loss: 0.5240 - val_acc: 0.9500\n",
            "Epoch 83/1000\n",
            "1/1 [==============================] - 0s 430ms/step - loss: 0.4717 - acc: 0.9500 - val_loss: 0.5740 - val_acc: 0.8500\n",
            "Epoch 84/1000\n",
            "1/1 [==============================] - 0s 432ms/step - loss: 0.6495 - acc: 0.9250 - val_loss: 0.7950 - val_acc: 0.9000\n",
            "Epoch 85/1000\n",
            "1/1 [==============================] - 0s 433ms/step - loss: 0.4744 - acc: 0.9250 - val_loss: 0.5522 - val_acc: 0.9500\n",
            "Epoch 86/1000\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 0.4506 - acc: 0.9500 - val_loss: 0.5272 - val_acc: 0.9000\n",
            "Epoch 87/1000\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 0.8872 - acc: 0.8250 - val_loss: 0.5629 - val_acc: 0.9500\n",
            "Epoch 88/1000\n",
            "1/1 [==============================] - 0s 432ms/step - loss: 0.5540 - acc: 0.9500 - val_loss: 0.6089 - val_acc: 0.9500\n",
            "Epoch 89/1000\n",
            "1/1 [==============================] - 0s 426ms/step - loss: 0.4701 - acc: 0.9000 - val_loss: 0.4476 - val_acc: 1.0000\n",
            "Epoch 90/1000\n",
            "1/1 [==============================] - 0s 426ms/step - loss: 0.7132 - acc: 0.9000 - val_loss: 0.5792 - val_acc: 0.8500\n",
            "Epoch 91/1000\n",
            "1/1 [==============================] - 0s 437ms/step - loss: 0.5308 - acc: 0.9500 - val_loss: 0.4294 - val_acc: 1.0000\n",
            "Epoch 92/1000\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 0.6478 - acc: 0.9000 - val_loss: 0.5990 - val_acc: 0.8500\n",
            "Epoch 93/1000\n",
            "1/1 [==============================] - 0s 441ms/step - loss: 0.7336 - acc: 0.9250 - val_loss: 0.5077 - val_acc: 0.9000\n",
            "Epoch 94/1000\n",
            "1/1 [==============================] - 0s 430ms/step - loss: 0.8281 - acc: 0.8250 - val_loss: 0.4419 - val_acc: 0.9500\n",
            "Epoch 95/1000\n",
            "1/1 [==============================] - 0s 435ms/step - loss: 1.3336 - acc: 0.7750 - val_loss: 0.4506 - val_acc: 0.9500\n",
            "Epoch 96/1000\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 0.5910 - acc: 0.9000 - val_loss: 0.4127 - val_acc: 1.0000\n",
            "Epoch 97/1000\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 0.7729 - acc: 0.8500 - val_loss: 0.5232 - val_acc: 0.9000\n",
            "Epoch 98/1000\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 0.5251 - acc: 0.9250 - val_loss: 0.4000 - val_acc: 0.9500\n",
            "Epoch 99/1000\n",
            "1/1 [==============================] - 0s 425ms/step - loss: 0.8104 - acc: 0.9000 - val_loss: 0.3804 - val_acc: 1.0000\n",
            "Epoch 100/1000\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 0.7436 - acc: 0.8500 - val_loss: 0.4276 - val_acc: 1.0000\n",
            "Epoch 101/1000\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 1.0017 - acc: 0.8500 - val_loss: 0.4879 - val_acc: 0.9500\n",
            "Epoch 102/1000\n",
            "1/1 [==============================] - 0s 444ms/step - loss: 1.1511 - acc: 0.7250 - val_loss: 0.4033 - val_acc: 1.0000\n",
            "Epoch 103/1000\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 0.7626 - acc: 0.8750 - val_loss: 0.4426 - val_acc: 0.9000\n",
            "Epoch 104/1000\n",
            "1/1 [==============================] - 0s 430ms/step - loss: 1.1290 - acc: 0.8250 - val_loss: 0.5330 - val_acc: 0.8000\n",
            "Epoch 105/1000\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 0.7708 - acc: 0.9000 - val_loss: 0.8407 - val_acc: 0.7000\n",
            "Epoch 106/1000\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 0.4615 - acc: 0.9750 - val_loss: 0.6444 - val_acc: 0.7500\n",
            "Epoch 107/1000\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 0.3891 - acc: 1.0000 - val_loss: 0.6231 - val_acc: 0.8500\n",
            "Epoch 108/1000\n",
            "1/1 [==============================] - 0s 430ms/step - loss: 0.4948 - acc: 0.9500 - val_loss: 0.4720 - val_acc: 0.9000\n",
            "Epoch 109/1000\n",
            "1/1 [==============================] - 0s 432ms/step - loss: 0.8105 - acc: 0.8500 - val_loss: 0.3980 - val_acc: 0.9500\n",
            "Epoch 110/1000\n",
            "1/1 [==============================] - 0s 432ms/step - loss: 0.5926 - acc: 0.9250 - val_loss: 0.6118 - val_acc: 0.8500\n",
            "Epoch 111/1000\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 0.7749 - acc: 0.9000 - val_loss: 0.8524 - val_acc: 0.7500\n",
            "Epoch 112/1000\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 0.5864 - acc: 0.9500 - val_loss: 0.5363 - val_acc: 0.8000\n",
            "Epoch 113/1000\n",
            "1/1 [==============================] - 0s 436ms/step - loss: 0.7899 - acc: 0.9000 - val_loss: 1.0964 - val_acc: 0.7000\n",
            "Epoch 114/1000\n",
            "1/1 [==============================] - 0s 432ms/step - loss: 0.4070 - acc: 0.9500 - val_loss: 0.7029 - val_acc: 0.7500\n",
            "Epoch 115/1000\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 0.4485 - acc: 0.9750 - val_loss: 0.5670 - val_acc: 0.8000\n",
            "Epoch 116/1000\n",
            "1/1 [==============================] - 0s 424ms/step - loss: 0.3906 - acc: 0.9750 - val_loss: 1.0268 - val_acc: 0.6500\n",
            "Epoch 117/1000\n",
            "1/1 [==============================] - 0s 427ms/step - loss: 0.5394 - acc: 0.9250 - val_loss: 0.5300 - val_acc: 0.8500\n",
            "Epoch 118/1000\n",
            "1/1 [==============================] - 0s 427ms/step - loss: 0.4384 - acc: 0.9500 - val_loss: 0.6677 - val_acc: 0.8000\n",
            "Epoch 119/1000\n",
            "1/1 [==============================] - 0s 432ms/step - loss: 0.7127 - acc: 0.9000 - val_loss: 0.5697 - val_acc: 0.8000\n",
            "Epoch 120/1000\n",
            "1/1 [==============================] - 0s 425ms/step - loss: 0.5474 - acc: 0.9250 - val_loss: 0.5545 - val_acc: 0.7500\n",
            "Epoch 121/1000\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 0.4921 - acc: 0.9500 - val_loss: 0.5965 - val_acc: 0.8000\n",
            "Epoch 122/1000\n",
            "1/1 [==============================] - 0s 435ms/step - loss: 0.3788 - acc: 0.9750 - val_loss: 0.6506 - val_acc: 0.7500\n",
            "Epoch 123/1000\n",
            "1/1 [==============================] - 0s 426ms/step - loss: 0.7101 - acc: 0.8750 - val_loss: 0.9274 - val_acc: 0.7000\n",
            "Epoch 124/1000\n",
            "1/1 [==============================] - 0s 439ms/step - loss: 0.5753 - acc: 0.9000 - val_loss: 0.4972 - val_acc: 0.8000\n",
            "Epoch 125/1000\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 0.3552 - acc: 0.9250 - val_loss: 0.4736 - val_acc: 0.9000\n",
            "Epoch 126/1000\n",
            "1/1 [==============================] - 0s 435ms/step - loss: 0.3502 - acc: 0.9750 - val_loss: 0.4141 - val_acc: 0.8500\n",
            "Epoch 127/1000\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 0.5034 - acc: 0.9250 - val_loss: 0.4066 - val_acc: 0.9500\n",
            "Epoch 128/1000\n",
            "1/1 [==============================] - 0s 438ms/step - loss: 0.4702 - acc: 0.9500 - val_loss: 0.3930 - val_acc: 0.9500\n",
            "Epoch 129/1000\n",
            "1/1 [==============================] - 0s 435ms/step - loss: 0.5688 - acc: 0.8750 - val_loss: 0.4341 - val_acc: 0.9000\n",
            "Epoch 130/1000\n",
            "1/1 [==============================] - 0s 430ms/step - loss: 0.5407 - acc: 0.9250 - val_loss: 0.5151 - val_acc: 0.8000\n",
            "Epoch 131/1000\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 0.5810 - acc: 0.9500 - val_loss: 0.5353 - val_acc: 0.8000\n",
            "Epoch 132/1000\n",
            "1/1 [==============================] - 0s 427ms/step - loss: 0.5298 - acc: 0.9000 - val_loss: 0.4074 - val_acc: 0.8500\n",
            "Epoch 133/1000\n",
            "1/1 [==============================] - 0s 433ms/step - loss: 1.0412 - acc: 0.8250 - val_loss: 0.5619 - val_acc: 0.8000\n",
            "Epoch 134/1000\n",
            "1/1 [==============================] - 0s 432ms/step - loss: 0.6360 - acc: 0.9000 - val_loss: 0.6232 - val_acc: 0.8500\n",
            "Epoch 135/1000\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 0.6157 - acc: 0.8750 - val_loss: 0.5553 - val_acc: 0.8500\n",
            "Epoch 136/1000\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 0.5870 - acc: 0.9000 - val_loss: 0.5775 - val_acc: 0.8000\n",
            "Epoch 137/1000\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 0.3328 - acc: 0.9500 - val_loss: 0.4634 - val_acc: 0.8500\n",
            "Epoch 138/1000\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 0.5512 - acc: 0.9250 - val_loss: 0.5194 - val_acc: 0.8000\n",
            "Epoch 139/1000\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 0.6825 - acc: 0.9250 - val_loss: 0.5844 - val_acc: 0.8000\n",
            "Epoch 140/1000\n",
            "1/1 [==============================] - 0s 430ms/step - loss: 0.6066 - acc: 0.9000 - val_loss: 0.5528 - val_acc: 0.8500\n",
            "Epoch 141/1000\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 0.4681 - acc: 0.9250 - val_loss: 0.4581 - val_acc: 0.9000\n",
            "Epoch 142/1000\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 0.5326 - acc: 0.9250 - val_loss: 0.6004 - val_acc: 0.8000\n",
            "Epoch 143/1000\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 0.8118 - acc: 0.8250 - val_loss: 0.5141 - val_acc: 0.8500\n",
            "Epoch 144/1000\n",
            "1/1 [==============================] - 0s 430ms/step - loss: 0.3582 - acc: 0.9500 - val_loss: 0.4417 - val_acc: 0.9000\n",
            "Epoch 145/1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YS7S14JK6l_t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Test de la commande vocale"
      ]
    },
    {
      "metadata": {
        "id": "f3dhxUS46lJK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DiMz-uT-sb1u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reférences\n",
        "\n",
        "*   https://www.dlology.com/blog/how-to-do-real-time-trigger-word-detection-with-keras/\n",
        "*   https://github.com/Jakobovski/free-spoken-digit-dataset\n",
        "*  https://github.com/datascienceinc/speech-commands-oow2018\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "TV0_mGzzYTBW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}